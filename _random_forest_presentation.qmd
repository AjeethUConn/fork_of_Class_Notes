
## A Primer on Random Forest
This section was prepared by Gaofei Zhang. 
I am a second year PhD student in Health Promotion Science. 

### Welcome to our presentation on **Random Forest**

In today's talk, we will explore what Random Forest is, why it matters,
and how to apply it to real-world data.

We will cover the basic concepts and practical steps for using a Random Forest regressor. 
In our example, we predict the number of persons injured in NYC crashes by using 
accident attributes.


## A Primer on Random Forest

Random Forest is a supervised ensemble learning method that builds multiple
decision trees and combines their outputs to improve predictive performance
and control overfitting. It is widely used for both classification and
regression tasks due to its robustness and interpretability.

### Conceptual Foundations

**1. Bootstrap Aggregating (Bagging)**
- For each tree, sample the training set with replacement (bootstrapping).
- This creates diverse datasets so trees are uncorrelated.

**2. Random Feature Selection**
- At each node, select a random subset of features when splitting.
- This further decorrelates trees, reducing variance.

**3. Aggregation of Predictions**
- **Classification**: each tree votes; the majority class wins.
- **Regression**: average the outputs of all trees.

**Advantages**
- Reduces overfitting compared to single trees.  
- Provides feature importance metrics.  
- Handles both numerical and categorical data well.

**Limitations**
- Computationally intensive for large forests.  
- Model size can be large, affecting memory usage.

#### Random Forest Principles

- **Ensemble Method**: Combines many decision trees.
- **Randomness**:  
  - *Bootstrap Sampling*: Each tree is trained on a random sample (with 
    replacement) of the dataset.
  - *Random Feature Selection*: At each split, only a subset of features is considered.
- **Voting/Averaging**: For classification, predictions are made by majority vote.


#### Parameter Tuning

- **Key Parameters**:  
  - `n_estimators`: Number of trees in the forest.  
  - `max_depth`: Maximum depth of each tree.  
  - `max_features`: Number of features to consider when looking for the best 
    split.
- **Tuning Process**:  
  Using methods like GridSearchCV, one can search for the optimal combination of these parameters to improve performance.
- **Justification**:  
  Tuning these parameters can further reduce overfitting, improve generalization, 
  and provide better insight into feature contributions.

#### Concrete Example: NYC Crash Data

This example demonstrates how to build a Random Forest regression model using 
accident attributes to predict "NUMBER OF PERSONS INJURED".


### 1. Data Preprocessing

We start by loading the NYC crash dataset and extracting the crash hour from
the crash time column. This transforms a string time into an integer hour.

```{python}
#| echo: true  
#| eval: true  
#| include: true
import pandas as pd
from pathlib import Path

# Build dynamic file path
dir_path = Path.cwd() / "data"
csv_file = dir_path / "nyccrashes_2024w0630_by20250212.csv"

# Load data and parse crash hour
def preprocess_data(path):
    df = pd.read_csv(path)
    df["CRASH HOUR"] = pd.to_datetime(
        df["CRASH TIME"], format="%H:%M",
        errors="coerce"
    ).dt.hour
    if df["CRASH HOUR"].isnull().any():
        med = df["CRASH HOUR"].median()
        df["CRASH HOUR"].fillna(med, inplace=True)
    print("Data shape:", df.shape)
    print(df.head())
    return df

# Execute preprocessing
df = preprocess_data(csv_file)
```
#### Result Explanation
- **Data shape** (1876, 30) – 1876 rows and 30 columns.
- **First 5 rows** show sample records.


### 2. Feature Engineering & Train/Test Split

Here we encode categorical features and combine with numeric ones, then
split into training and testing subsets.

```{python}
#| echo: true  
#| eval: true  
#| include: true
import numpy as np
from sklearn.model_selection import train_test_split

# Create features X and target y
def feature_engineering(df):
    df.columns = df.columns.str.strip()
    df = df.dropna(
        subset=["BOROUGH", "ZIP CODE"]
    )
    df["BOROUGH"] = df["BOROUGH"].str.upper()
    df["ZIP CODE"] = (
        df["ZIP CODE"].astype(int)
        .astype(str)
    )
    cats = pd.get_dummies(
        df[["BOROUGH", "ZIP CODE"]],
        drop_first=True
    )
    nums = df[["LATITUDE", "LONGITUDE",
               "CRASH HOUR"]]
    X = pd.concat([cats, nums], axis=1)
    y = pd.to_numeric(
        df["NUMBER OF PERSONS INJURED"],
        errors="coerce"
    ).fillna(0)
    return X, y

# Execute feature engineering
df_X, df_y = feature_engineering(df)
print("Feature matrix shape:", df_X.shape)
X_train, X_test, y_train, y_test = (
    train_test_split(
        df_X, df_y,
        test_size=0.3,
        random_state=42
    )
)
print("Train set:", X_train.shape)
print("Test set:", X_test.shape)
```

Results will be like:

- Feature matrix shape: (1334, 174)\
- Train set size: (933, 174)\
- Test set size: (401, 174)\

#### Result Explanation
- Feature matrix shape: (1334, 174)\

    1334: This is the number of samples (or rows) in the feature matrix.

    174: This is the number of features (or columns) in the matrix.

- Train set size: (933, 174)\

    933: This indicates that there are 933 samples in the training set.

    174: Each sample in the training set has 174 features.

- Test set size: (401, 174)\

    401: This indicates that there are 401 samples in the test set.

    174: Each sample in the test set has 174 features.


### 3. Parameter Tuning & Model Training

We use grid search to find optimal hyperparameters and then evaluate the model
on the test set.

```{python}
#| echo: true  
#| eval: true  
#| include: true
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    r2_score, mean_squared_error
)

# Tune hyperparameters and train model
def tune_and_train(
    X_tr, y_tr, X_te, y_te
):
    param_grid = {
        "n_estimators": [500, 1000, 2000],
        "max_depth": [None, 10, 20],
        "max_features": ["sqrt", "log2"]
    }
    rf = RandomForestRegressor(
        random_state=42
    )
    gs = GridSearchCV(
        rf, param_grid,
        cv=5, n_jobs=-1,
        scoring="r2"
    )
    gs.fit(X_tr, y_tr)
    print("Best params:", gs.best_params_)
    print("CV R²:", gs.best_score_)
    model = gs.best_estimator_
    preds = model.predict(X_te)
    print("Test R²:", r2_score(y_te, preds))
    print("Test MSE:", mean_squared_error(
        y_te, preds
    ))
    return model

# Execute tuning and training
rf_model = tune_and_train(
    X_train, y_train,
    X_test, y_test
)
```

Results will be like:

- Best Parameters: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 2000}\
- Best cross-validation R² score: -0.01704506305455513\
- Random Forest R² Score: -0.023889832065792982\
- Random Forest Mean Squared Error: 0.8725932213499686\


### 4. Visualize Feature Importance

Plot the importance of each feature to interpret the model.

```{python}
#| echo: true  
#| eval: true  
#| include: true
import matplotlib.pyplot as plt

# Plot feature importances
def plot_importance(
    model, names
):
    imp = model.feature_importances_
    order = np.argsort(imp)[::-1]
    plt.figure(figsize=(8, 4))
    plt.bar(
        range(len(imp)), imp[order],
        align="center"
    )
    plt.xticks(
        range(len(imp)),
        [names[i] for i in order],
        rotation=45
    )
    plt.title("Feature Importance")
    plt.tight_layout()
    plt.show()

# Execute visualization
plot_importance(
    rf_model, df_X.columns.tolist()
)
```

#### Result Explanation
- The printed values and the bar chart visually show the importance of each feature, 
highlighting that geographic location and crash time are key predictors.\

Feature Importances:\
  LATITUDE: 0.2243\
  LONGITUDE: 0.1978\
  CRASH HOUR: 0.1854\
  
### 5. Summary & Further Reading

- **Key takeaways**: Random Forest reduces variance by bagging and random
  feature selection.  
- **Important features**: `LATITUDE`, `LONGITUDE`, and `CRASH HOUR`.  

**Further Reading:**  
1. Breiman, L. (2001). _Random Forests_.  
2. Scikit-learn docs: `RandomForestRegressor`.  
3. Quarto docs for reproducibility.  

— End of section —
